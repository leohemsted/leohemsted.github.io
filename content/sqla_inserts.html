<h1>Speeding up sqlalchemy inserts, or how I can't come up with a sexy blog title</h1>

<p>
A recent project I was involved in included refactoring a bespoke bin-packing algorithm that had, through continual design changes and rework, become unwieldy and unmaintainable. The decision was made to spin it out into a separate microservice (from the main monolithic python webserver). We took this approach for a few reasons:

<ul>
    <li>By forcing separation from the database we hoped to encourage cleaner implementation</li>
    <li>Greater extensibility thanks to separate release cycles</li>
    <li>General separation of concerns</li>
    <li>A pinch of curiosity</li>
    <li>Plenty of other microservice benefits that I won't go into here</li>
</ul>

However, during the switch, we ran into a few problems around memory and performance when consuming a large amount of json data from an HTTP response, and inserting it into a PostgreSQL database. Here's how that went down.
</p>

<h2> The beginning </h2>

<p>
We duly set about splitting out the code, and created a new endpoint <code>/bin_pack</code> which used requests to call off to our new microservice.
</p>

<pre>
    <code class="language-python">
    data = {
        'widgets': [...],
        'bins': [...]
    }
    resp = requests.post(microservice_url, json=data)
    process_response(resp.json())
    </code>
</pre>

<p>
This code worked fine during early testing, however, when we tried to scale up our systems we started running into problems. Our bin-filling algorithm results in an incredibly large amount of bins, all containing 10 to 20 or so small structures. The data returned from the service looked something along the lines of
</p>

<pre>
    <code class="language-json">
    {
        "bins": [
            {
                "bin_uuid": "00000000-0000-0000-0000-000000000000",
                "items": [
                    {
                        "uuid": "00000000-0000-0000-0000-000000000000",
                        "other_uuid": "00000000-0000-0000-0000-000000000000",
                    },
                    ... * ~10
                ]
            },
            ... * ~100,000 (one bin per showing for two weeks in a large cinema chain)
        ]
    }
    </code>
</pre>

<p>
Once this data (~100mb) returned to the main monolith, we set about processing it. There isn't much to do with it other than put it straight into our database, so we went through several iterations. First, the totally naive approach that we initially prototyped
</p>


<pre>
    <code class="language-python">
    ...
    for bin in data['bins']:
        for item in bin['items']:
            db.Session.add(db.Thing(
                uuid=item['uuid'],
                other_uuid=item['other_uuid'],
                bin_uuid=bin['bin_uuid']
            ))
    db.Session.commit()
    # Processing took 15.42s
    # Committing took 58.65s
    </code>
</pre>

<p>
Yikes! We generated a 20,000 bin test data set and threw in some straightforward time logging, and the results weren't pretty. It took almost 75 seconds to actually get that data into the database. This clearly wasn't going to scale nicely up to five times the data-set, so what could we do? Luckily, SQLAlchemy provides us with a few avenues of attack.
</p>

<p>
Firstly, SQLAlchemy 1.0 provided some brand new <a href="http://docs.sqlalchemy.org/en/rel_1_0/orm/persistence_techniques.html#bulk-operations">bulk operations</a>, which seemed perfect for our usecase. Plugging them in was pretty straightforward, too.
</p>


<pre>
    <code class="language-python">
    things = []
    for bin in data['bins']:
        things += [
            db.Thing(
                uuid=item['uuid'],
                other_uuid=item['other_uuid'],
                bin_uuid=bin['bin_uuid']
            )
            for item in bin['items']
        ]
    db.Session.bulk_save_objects(things)
    # Processing took 9.48s
    # Committing took 46.53s
    # Entire process took 56.02s
    </code>
</pre>

<p>
Closer to the bone still, we can get rid of a lot of the ORM constructs by looking at SQLAlchemy core. The ORM adds a lot of cruft around making objects more useable and more aligned with OO patterns, so by getting away from that and closer to the core ORM we hoped to cut out a lot of time.
</p>

<pre>
    <code class="language-python">
    bins = []
    things = []
    for i, bin in enumerate(data['bins']):
        things += [
            {
                'uuid': item['uuid'],
                'other_uuid': item['other_uuid'],
                'bin_uuid': bin['bin_uuid']
            }
            for item in bin['items']
        ]
    db.engine.execute(
        db.Thing.__table__.insert(),
        things
    )
    # Processing took 0.18s
    # Committing took 42.46s
    # Entire process took 42.64s
    </code>
</pre>

<p>
We're getting better, but it's still a lot longer than I'd like! - and 80% of the time is hitting the DB. Luckily, we're using Postgres, which has the delightful <a href="http://www.postgresql.org/docs/current/interactive/populate.html#POPULATE-COPY-FROM
">COPY FROM</a> command, designed for populating large swathes of homogenous data (as opposed to the much more flexible standard ANSI SQL insert statement). It takes in a delimited filelike object and an array of column names, and pipes them straight into the database.
</p>

<pre>
    <code class="language-python">
    string_io = io.StringIO()
    for bin in data['bins']:
        for item in bin['items']:
            # write a tab separated file to the stringio
            string_io.write(
                '\t'.join([
                    item['uuid'],
                    item['other_uuid'],
                    bin['bin_uuid']
                ]) + '\n'
            )
    # rewind
    string_io.seek(0)

    connection = db.engine.raw_connection()
    cur = connection.cursor()
    cur.copy_from(
        string_io,  # filelike obj
        'thing',  # table name
        sep='\t',  # separator
        columns=['uuid', 'other_uuid', 'bin_uuid']
    )
    connection.commit()
    connection.close()
    # Processing took 0.29s
    # Copy From took 7.99s
    # Entire process took 8.28s
    </code>
</pre>

<p>
A huge saving! We've gone from 60 seconds of processing down to just 8.3 seconds! We weren't too enthusastic about mucking about with raw cursors and TSVs, but you're never going to perform large scale sql operations without getting your hands somewhat dirty, and a performance increase of almost an order of magnitude is undeniably huge. We knew we weren't out of the woods yet, but that was certainly as far as we were going to get without looking at the rest of the request to the bin-fill microservice... (to be continued)
</p>

<hr/>
<h3>Further reading:</h3>
<ul>
    <li><sup id="fn1">1. Phil Calcaldo's excellent blog post <a href="http://philcalcado.com/2015/09/08/how_we_ended_up_with_microservices.html" target="_blank">"How we ended up with microservices"</a> is a fantastic read if you're curious <a href="#ref1" title="">â†©</a></sup>
    </li>
    <li><sup id="fn1">2. Mobify had a great article on <a href="http://www.mobify.com/blog/sqlalchemy-memory-magic/">reducing memory with sqlalchemy</a> if you're interested in some of the ways to improve sqlalchemy performance, more focused at querying large sets of data. This one also proved invaluable.
    </li>
</ul>
